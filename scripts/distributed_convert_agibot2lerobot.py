""" 
This project is built upon the open-source project ğŸ¤— LeRobot: https://github.com/huggingface/lerobot 

We are grateful to the LeRobot team for their outstanding work and their contributions to the community. 

If you find this project useful, please also consider supporting and exploring LeRobot. 

This is an optimized version for processing large-scale datasets with multi-node and NUMA-aware parallelism.
"""

import os
import json
import shutil
import logging
import argparse
import gc
import time
import signal
import socket
import resource
import psutil
import traceback
import multiprocessing as mp
from pathlib import Path
from typing import Callable, Optional, List, Dict, Any, Tuple, Union
from functools import partial
from math import ceil
from copy import deepcopy
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed

import h5py
import torch
import einops
import numpy as np
from PIL import Image
from tqdm import tqdm
from tqdm.contrib.concurrent import process_map
from lerobot.common.datasets.lerobot_dataset import LeRobotDataset
from lerobot.common.datasets.utils import (
    STATS_PATH,
    check_timestamps_sync,
    get_episode_data_index,
    serialize_dict,
    write_json,
)

# ================== æ–°å¢å…±äº«å­˜å‚¨é…ç½® ==================
SHARED_STORAGE = Path("/fs-computility/efm/shared")
# é…ç½®æ‰€æœ‰ç¼“å­˜å’Œä¸´æ—¶æ–‡ä»¶çš„å­ç›®å½•
CACHE_ROOT = SHARED_STORAGE / "agibot2lerobot_cache"
os.environ.update({
    # Hugging Face æ•°æ®é›†ç¼“å­˜
    "HF_DATASETS_CACHE": str(CACHE_ROOT / "huggingface/datasets"),
    # PyTorchç¼“å­˜
    "TORCH_HOME": str(CACHE_ROOT / "torch"),
    # ç³»ç»Ÿä¸´æ—¶æ–‡ä»¶
    "TMPDIR": str(CACHE_ROOT / "tmp"),
    # ffmpegä¸´æ—¶æ–‡ä»¶
    "FFMPEG_TEMP": str(CACHE_ROOT / "ffmpeg")
})

# ä¿å­˜å…ˆå‰å®šä¹‰çš„å¸¸é‡
HEAD_COLOR = "head_color.mp4"
HAND_LEFT_COLOR = "hand_left_color.mp4"
HAND_RIGHT_COLOR = "hand_right_color.mp4"
HEAD_DEPTH = "head_depth"

DEFAULT_IMAGE_PATH = (
    "images/{image_key}/episode_{episode_index:06d}/frame_{frame_index:06d}.jpg"
)

# ä¿æŒåŸå§‹çš„ç‰¹å¾å®šä¹‰
FEATURES = {
    "observation.images.top_head": {
        "dtype": "video",
        "shape": [480, 640, 3],
        "names": ["height", "width", "channel"],
        "video_info": {
            "video.fps": 30.0,
            "video.codec": "av1",
            "video.pix_fmt": "yuv420p",
            "video.is_depth_map": False,
            "has_audio": False,
        },
    },
    "observation.images.cam_top_depth": {
        "dtype": "image",
        "shape": [480, 640, 1],
        "names": ["height", "width", "channel"],
    },
    "observation.images.hand_left": {
        "dtype": "video",
        "shape": [480, 640, 3],
        "names": ["height", "width", "channel"],
        "video_info": {
            "video.fps": 30.0,
            "video.codec": "av1",
            "video.pix_fmt": "yuv420p",
            "video.is_depth_map": False,
            "has_audio": False,
        },
    },
    "observation.images.hand_right": {
        "dtype": "video",
        "shape": [480, 640, 3],
        "names": ["height", "width", "channel"],
        "video_info": {
            "video.fps": 30.0,
            "video.codec": "av1",
            "video.pix_fmt": "yuv420p",
            "video.is_depth_map": False,
            "has_audio": False,
        },
    },
    "observation.state": {
        "dtype": "float32",
        "shape": [20],
    },
    "action": {
        "dtype": "float32",
        "shape": [22],
    },
    "episode_index": {
        "dtype": "int64",
        "shape": [1],
        "names": None,
    },
    "frame_index": {
        "dtype": "int64",
        "shape": [1],
        "names": None,
    },
    "index": {
        "dtype": "int64",
        "shape": [1],
        "names": None,
    },
    "task_index": {
        "dtype": "int64",
        "shape": [1],
        "names": None,
    },
}

# ================== èµ„æºç®¡ç†ä¸ç›‘æ§ ==================
class ResourceMonitor:
    """ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µçš„ç±»"""
    def __init__(self, log_interval=60, max_memory_percent=90, log_file=None):
        self.log_interval = log_interval
        self.max_memory_percent = max_memory_percent
        self.running = False
        self.thread = None
        self.log_file = log_file
        self.start_time = time.time()
    
    def _log_message(self, message):
        """è®°å½•æ¶ˆæ¯åˆ°æ—¥å¿—å’Œæ–‡ä»¶"""
        logging.info(message)
        if self.log_file:
            with open(self.log_file, "a") as f:
                timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
                f.write(f"{timestamp} - {message}\n")
    
    def _monitor_loop(self):
        """èµ„æºç›‘æ§å¾ªç¯"""
        while self.running:
            try:
                # è·å–èµ„æºä½¿ç”¨æƒ…å†µ
                memory_usage = psutil.virtual_memory()
                disk_usage = psutil.disk_usage(os.getcwd())
                cpu_percent = psutil.cpu_percent(interval=1)
                
                # è®¡ç®—è¿è¡Œæ—¶é—´
                elapsed = time.time() - self.start_time
                hours, remainder = divmod(elapsed, 3600)
                minutes, seconds = divmod(remainder, 60)
                
                message = (
                    f"è¿è¡Œæ—¶é—´: {int(hours)}h {int(minutes)}m {int(seconds)}s | "
                    f"å†…å­˜: {memory_usage.used/(1024**3):.1f}GB/{memory_usage.total/(1024**3):.1f}GB "
                    f"({memory_usage.percent:.1f}%) | "
                    f"ç£ç›˜: {disk_usage.used/(1024**3):.1f}GB/{disk_usage.total/(1024**3):.1f}GB "
                    f"({disk_usage.percent:.1f}%) | "
                    f"CPU: {cpu_percent:.1f}%"
                )
                self._log_message(message)
                
                # æ£€æŸ¥æ˜¯å¦æ¥è¿‘èµ„æºä¸Šé™
                if memory_usage.percent > self.max_memory_percent:
                    self._log_message(f"è­¦å‘Š: å†…å­˜ä½¿ç”¨ç‡è¾¾åˆ°{memory_usage.percent:.1f}%ï¼Œè¶…è¿‡{self.max_memory_percent}%é˜ˆå€¼")
                    # è¿™é‡Œå¯ä»¥æ·»åŠ ä¸€äº›ç´§æ€¥å¤„ç†ï¼Œå¦‚è§¦å‘GCæˆ–æš‚åœæŸäº›ä»»åŠ¡
                    gc.collect()
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                
                # ä¼‘çœ æŒ‡å®šæ—¶é—´
                time.sleep(self.log_interval)
            except Exception as e:
                self._log_message(f"ç›‘æ§çº¿ç¨‹å¼‚å¸¸: {str(e)}")
                time.sleep(self.log_interval * 2)  # å‡ºé”™æ—¶ç­‰å¾…æ›´é•¿æ—¶é—´
    
    def start(self):
        """å¯åŠ¨ç›‘æ§çº¿ç¨‹"""
        if not self.running:
            self.running = True
            self.thread = ThreadPoolExecutor(max_workers=1)
            self.thread.submit(self._monitor_loop)
            self._log_message("èµ„æºç›‘æ§å·²å¯åŠ¨")
    
    def stop(self):
        """åœæ­¢ç›‘æ§çº¿ç¨‹"""
        if self.running:
            self.running = False
            if self.thread:
                self.thread.shutdown(wait=False)
            self._log_message("èµ„æºç›‘æ§å·²åœæ­¢")

def get_memory_usage_gb():
    """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡ï¼ˆGBï¼‰"""
    return psutil.virtual_memory().used / (1024**3)

def get_disk_usage_gb(path):
    """è·å–æŒ‡å®šè·¯å¾„çš„ç£ç›˜ä½¿ç”¨é‡ï¼ˆGBï¼‰"""
    return psutil.disk_usage(path).used / (1024**3)

def get_system_total_memory_gb():
    """è·å–ç³»ç»Ÿæ€»å†…å­˜ï¼ˆGBï¼‰"""
    return psutil.virtual_memory().total / (1024**3)

# ================== NUMAæ‹“æ‰‘æ„ŸçŸ¥ ==================
def get_numa_topology():
    """è·å–NUMAèŠ‚ç‚¹æ‹“æ‰‘ç»“æ„"""
    try:
        numa_info = {}
        # åœ¨å®é™…ç¯å¢ƒä¸­ï¼Œåº”è¯¥ä½¿ç”¨æ›´ç²¾ç¡®çš„æ–¹æ³•è·å–NUMAé…ç½®
        # è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„æ–¹æ³•ï¼Œå‡è®¾æœ‰2ä¸ªNUMAèŠ‚ç‚¹ï¼Œæ¯ä¸ªèŠ‚ç‚¹åˆ†é…ä¸€åŠçš„CPUæ ¸å¿ƒ
        total_cores = os.cpu_count()
        cores_per_node = total_cores // 2
        
        numa_info[0] = list(range(0, cores_per_node))
        numa_info[1] = list(range(cores_per_node, total_cores))
        
        return numa_info
    except Exception as e:
        logging.warning(f"æ— æ³•è·å–NUMAæ‹“æ‰‘: {e}, ä½¿ç”¨é»˜è®¤é…ç½®")
        return {0: list(range(os.cpu_count()))}

def optimize_parallelism(memory_gb=None):
    """ä¼˜åŒ–å¹¶è¡Œåº¦é…ç½®"""
    total_cores = os.cpu_count()
    if memory_gb is None:
        memory_gb = get_system_total_memory_gb()
    
    # è®¡ç®—æœ€ä¼˜é…ç½®
    # å‡è®¾æ¯ä¸ªå¤„ç†ä»»åŠ¡éœ€è¦çº¦20GBå†…å­˜
    num_processes = min(max(1, total_cores // 8), max(1, int(memory_gb // 20)))
    threads_per_process = max(2, min(8, total_cores // num_processes))
    
    return num_processes, threads_per_process

# ================== IOä¼˜åŒ– ==================
def setup_io_optimizations():
    """è®¾ç½®IOä¼˜åŒ–"""
    # å¢åŠ æ–‡ä»¶æè¿°ç¬¦é™åˆ¶
    try:
        soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)
        new_soft = min(hard, 65536)  # è®¾ç½®ä¸€ä¸ªåˆç†çš„é«˜å€¼
        resource.setrlimit(resource.RLIMIT_NOFILE, (new_soft, hard))
        logging.info(f"æ–‡ä»¶æè¿°ç¬¦é™åˆ¶è®¾ç½®ä¸º: {new_soft}")
    except Exception as e:
        logging.warning(f"æ— æ³•è°ƒæ•´æ–‡ä»¶æè¿°ç¬¦é™åˆ¶: {e}")
    
    # é…ç½®HDF5ä¸ä½¿ç”¨æ–‡ä»¶é”
    os.environ["HDF5_USE_FILE_LOCKING"] = "FALSE"
    
    # å…³é—­PyTorchçš„çº¿ç¨‹ä¼˜åŒ–ï¼Œé¿å…å’Œæˆ‘ä»¬çš„å¹¶è¡Œç­–ç•¥å†²çª
    torch.set_num_threads(1)

# ================== æ–­ç‚¹ç»­ä¼ æœºåˆ¶ ==================
def setup_directories():
    """åˆå§‹åŒ–å…±äº«å­˜å‚¨ç›®å½•ç»“æ„"""
    required_dirs = [
        CACHE_ROOT / "huggingface/datasets",
        CACHE_ROOT / "torch",
        CACHE_ROOT / "tmp",
        CACHE_ROOT / "ffmpeg",
        CACHE_ROOT / "multiprocessing",
        CACHE_ROOT / "progress_tracking"  # æ–°å¢è¿›åº¦è·Ÿè¸ªç›®å½•
    ]
    for d in required_dirs:
        d.mkdir(parents=True, exist_ok=True)
        # è®¾ç½®å®½æ¾æƒé™
        os.chmod(d, 0o777)

def get_progress_tracking_dir(repo_id):
    """è·å–è¿›åº¦è·Ÿè¸ªç›®å½•"""
    return CACHE_ROOT / "progress_tracking" / repo_id.replace('/', '_')

def get_task_state_file(repo_id, task_id):
    """è·å–ä»»åŠ¡çŠ¶æ€æ–‡ä»¶è·¯å¾„"""
    tracking_dir = get_progress_tracking_dir(repo_id)
    tracking_dir.mkdir(parents=True, exist_ok=True)
    return tracking_dir / f"task_{task_id}_state.json"

def get_processing_state(state_file):
    """è¯»å–å¤„ç†çŠ¶æ€"""
    if state_file.exists():
        with open(state_file, 'r') as f:
            try:
                return json.load(f)
            except json.JSONDecodeError:
                logging.warning(f"çŠ¶æ€æ–‡ä»¶ {state_file} æŸåï¼Œåˆ›å»ºæ–°çŠ¶æ€")
                return {"completed_episodes": [], "in_progress": False, "error_episodes": []}
    return {"completed_episodes": [], "in_progress": False, "error_episodes": []}

def update_processing_state(state_file, episode_id=None, start=False, complete=False, error=False):
    """æ›´æ–°å¤„ç†çŠ¶æ€"""
    state = get_processing_state(state_file)
    
    if start:
        state["in_progress"] = True
    
    if complete:
        state["in_progress"] = False
    
    if episode_id is not None:
        if error:
            if episode_id not in state["error_episodes"]:
                state["error_episodes"].append(episode_id)
        else:
            if episode_id not in state["completed_episodes"]:
                state["completed_episodes"].append(episode_id)
    
    # åŸå­å†™å…¥
    temp_file = state_file.with_suffix('.tmp')
    with open(temp_file, 'w') as f:
        json.dump(state, f, indent=2)
    temp_file.rename(state_file)
    
    return state

def get_all_tasks_progress(repo_id, task_ids):
    """è·å–æ‰€æœ‰ä»»åŠ¡çš„è¿›åº¦æƒ…å†µ"""
    progress = {}
    tracking_dir = get_progress_tracking_dir(repo_id)
    
    for task_id in task_ids:
        state_file = get_task_state_file(repo_id, task_id)
        if state_file.exists():
            state = get_processing_state(state_file)
            progress[task_id] = {
                "completed": len(state["completed_episodes"]),
                "errors": len(state["error_episodes"]),
                "in_progress": state["in_progress"]
            }
        else:
            progress[task_id] = {"completed": 0, "errors": 0, "in_progress": False}
    
    return progress

def get_task_episodes_count(src_path, task_id):
    """è·å–ä»»åŠ¡ä¸­çš„æ€»episodeæ•°é‡"""
    episodes_dir = Path(src_path) / "observations" / str(task_id)
    if not episodes_dir.exists():
        return 0
    
    episode_dirs = [d for d in episodes_dir.iterdir() if d.is_dir()]
    return len(episode_dirs)

# ================== æ•°æ®åŠ è½½ä¸å¤„ç† ==================
def load_depths(root_dir: str, camera_name: str):
    """åŠ è½½æ·±åº¦å›¾åƒæ•°æ®"""
    cam_path = Path(root_dir)
    all_imgs = sorted(list(cam_path.glob(f"{camera_name}*")))
    
    # ä½¿ç”¨å†…å­˜æ˜ å°„ä¼˜åŒ–å¤§æ–‡ä»¶åŠ è½½
    depth_imgs = []
    for f in all_imgs:
        try:
            # ä½¿ç”¨PILå’Œnumpyé«˜æ•ˆåŠ è½½å›¾åƒ
            img = np.array(Image.open(f)).astype(np.float32) / 1000
            depth_imgs.append(img)
        except Exception as e:
            logging.warning(f"åŠ è½½å›¾åƒ {f} å¤±è´¥: {str(e)}")
    
    return depth_imgs

def load_local_dataset(episode_id: int, src_path: str, task_id: int) -> Optional[tuple]:
    """åŠ è½½æœ¬åœ°æ•°æ®é›†å¹¶è¿”å›è§‚å¯Ÿå’ŒåŠ¨ä½œçš„å­—å…¸"""
    try:
        ob_dir = Path(src_path) / f"observations/{task_id}/{episode_id}"
        depth_imgs = load_depths(ob_dir / "depth", HEAD_DEPTH)
        proprio_dir = Path(src_path) / f"proprio_stats/{task_id}/{episode_id}"

        # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ‰“å¼€HDF5æ–‡ä»¶
        with h5py.File(proprio_dir / "proprio_stats.h5") as f:
            # é‡‡ç”¨å»¶è¿ŸåŠ è½½ç­–ç•¥ï¼Œåªåœ¨éœ€è¦æ—¶è¯»å–æ•°æ®
            state_joint = np.array(f["state/joint/position"])
            state_effector = np.array(f["state/effector/position"])
            state_head = np.array(f["state/head/position"])
            state_waist = np.array(f["state/waist/position"])
            action_joint = np.array(f["action/joint/position"])
            action_effector = np.array(f["action/effector/position"])
            action_head = np.array(f["action/head/position"])
            action_waist = np.array(f["action/waist/position"])
            action_velocity = np.array(f["action/robot/velocity"])

        states_value = np.hstack(
            [state_joint, state_effector, state_head, state_waist]
        ).astype(np.float32)
        
        action_value = np.hstack(
            [action_joint, action_effector, action_head, action_waist, action_velocity]
        ).astype(np.float32)

        # æ·»åŠ æ•°æ®æ ¡éªŒ
        if not (len(depth_imgs) == len(states_value) == len(action_value)):
            logging.warning(f"æ•°æ®é•¿åº¦ä¸åŒ¹é…ï¼Œepisode {episode_id}")
            return None

        frames = [
            {
                "observation.images.cam_top_depth": depth_imgs[i],
                "observation.state": states_value[i],
                "action": action_value[i],
            }
            for i in range(len(depth_imgs))
        ]

        v_path = ob_dir / "videos"
        videos = {
            "observation.images.top_head": v_path / HEAD_COLOR,
            "observation.images.hand_left": v_path / HAND_LEFT_COLOR,
            "observation.images.hand_right": v_path / HAND_RIGHT_COLOR,
        }
        return (frames, videos)
    except Exception as e:
        logging.error(f"åŠ è½½episode {episode_id} å¤±è´¥: {str(e)}")
        return None

def get_task_instruction(task_json_path: str) -> str:
    """è·å–ä»»åŠ¡è¯­è¨€æŒ‡ä»¤"""
    try:
        with open(task_json_path, "r") as f:
            task_info = json.load(f)
        if not task_info or not isinstance(task_info, list):
            raise ValueError("æ— æ•ˆçš„ä»»åŠ¡ä¿¡æ¯æ ¼å¼")
            
        task_name = task_info[0].get("task_name", "")
        task_init_scene = task_info[0].get("init_scene_text", "")
        return f"{task_name}.{task_init_scene}"
    except Exception as e:
        logging.error(f"åŠ è½½ä»»åŠ¡æŒ‡ä»¤å¤±è´¥: {str(e)}")
        return "unknown_task"

# ================== æ•°æ®é›†ç±»ç»§æ‰¿ ==================
class AgiBotDataset(LeRobotDataset):
    """å¢å¼ºçš„AgiBotDatasetï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œé”™è¯¯æ¢å¤"""
    
    def __init__(
        self,
        repo_id: str,
        root: str | Path | None = None,
        episodes: list[int] | None = None,
        image_transforms: Callable | None = None,
        delta_timestamps: dict[list[float]] | None = None,
        tolerance_s: float = 1e-4,
        download_videos: bool = True,
        local_files_only: bool = False,
        video_backend: str | None = None,
    ):
        super().__init__(
            repo_id=repo_id,
            root=root,
            episodes=episodes,
            image_transforms=image_transforms,
            delta_timestamps=delta_timestamps,
            tolerance_s=tolerance_s,
            download_videos=download_videos,
            local_files_only=local_files_only,
            video_backend=video_backend,
        )

    def save_episode(
        self, task: str, episode_data: dict | None = None, videos: dict | None = None
    ) -> None:
        """
        é‡å†™æ­¤æ–¹æ³•ä»¥å¤åˆ¶mp4è§†é¢‘åˆ°ç›®æ ‡ä½ç½®ï¼Œå¹¶å¢å¼ºé”™è¯¯å¤„ç†
        """
        try:
            if not episode_data:
                episode_buffer = self.episode_buffer
            else:
                episode_buffer = episode_data

            episode_length = episode_buffer.pop("size")
            episode_index = episode_buffer["episode_index"]
            
            if episode_index != self.meta.total_episodes:
                # TODO(aliberts): æ·»åŠ é€‰é¡¹ä»¥ä½¿ç”¨ç°æœ‰çš„episode_index
                raise NotImplementedError(
                    "æ‚¨å¯èƒ½æ‰‹åŠ¨æä¾›äº†episode_bufferï¼Œå…¶episode_indexä¸æ•°æ®é›†ä¸­çš„æ€»episodeæ•°ä¸åŒ¹é…ã€‚ç›®å‰ä¸æ”¯æŒæ­¤åŠŸèƒ½ã€‚"
                )

            if episode_length == 0:
                raise ValueError(
                    "å¿…é¡»åœ¨è°ƒç”¨`add_episode`ä¹‹å‰ä½¿ç”¨`add_frame`æ·»åŠ ä¸€ä¸ªæˆ–å¤šä¸ªå¸§"
                )

            task_index = self.meta.get_task_index(task)

            if not set(episode_buffer.keys()) == set(self.features):
                key_diff = set(self.features) - set(episode_buffer.keys())
                raise ValueError(f"episode_bufferç¼ºå°‘ä»¥ä¸‹é”®: {key_diff}")

            for key, ft in self.features.items():
                if key == "index":
                    episode_buffer[key] = np.arange(
                        self.meta.total_frames, self.meta.total_frames + episode_length
                    )
                elif key == "episode_index":
                    episode_buffer[key] = np.full((episode_length,), episode_index)
                elif key == "task_index":
                    episode_buffer[key] = np.full((episode_length,), task_index)
                elif ft["dtype"] in ["image", "video"]:
                    continue
                elif len(ft["shape"]) == 1 and ft["shape"][0] == 1:
                    episode_buffer[key] = np.array(episode_buffer[key], dtype=ft["dtype"])
                elif len(ft["shape"]) == 1 and ft["shape"][0] > 1:
                    episode_buffer[key] = np.stack(episode_buffer[key])
                elif len(ft["shape"]) == 2:  # å¤„ç†äºŒç»´æ•°ç»„
                    episode_buffer[key] = np.stack(episode_buffer[key])
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„ç‰¹å¾å½¢çŠ¶: {key}: {ft['shape']}")

            self._wait_image_writer()
            self._save_episode_table(episode_buffer, episode_index)

            self.meta.save_episode(episode_index, episode_length, task, task_index)
            
            # å¤åˆ¶è§†é¢‘æ–‡ä»¶
            for key in self.meta.video_keys:
                video_path = self.root / self.meta.get_video_file_path(episode_index, key)
                episode_buffer[key] = video_path
                video_path.parent.mkdir(parents=True, exist_ok=True)
                
                # éªŒè¯æºè§†é¢‘æ˜¯å¦å­˜åœ¨
                if key not in videos or not Path(videos[key]).exists():
                    logging.warning(f"è§†é¢‘æºæ–‡ä»¶ä¸å­˜åœ¨: {videos.get(key, 'None')}")
                    continue
                    
                shutil.copyfile(videos[key], video_path)
                
            if not episode_data:  # é‡ç½®ç¼“å†²åŒº
                self.episode_buffer = self.create_episode_buffer()
                
            self.consolidated = False
            return True
            
        except Exception as e:
            logging.error(f"ä¿å­˜episodeå¤±è´¥: {str(e)}\n{traceback.format_exc()}")
            return False

    def consolidate(
        self, run_compute_stats: bool = True, keep_image_files: bool = False
    ) -> None:
        """
        å¢å¼ºçš„consolidateæ–¹æ³•ï¼Œæ·»åŠ æ›´å¤šçš„é”™è¯¯å¤„ç†
        """
        try:
            logging.info("å¼€å§‹æ•´åˆæ•°æ®é›†...")
            self.hf_dataset = self.load_hf_dataset()
            self.episode_data_index = get_episode_data_index(
                self.meta.episodes, self.episodes
            )
            
            logging.info("æ£€æŸ¥æ—¶é—´æˆ³åŒæ­¥...")
            check_timestamps_sync(
                self.hf_dataset, self.episode_data_index, self.fps, self.tolerance_s
            )
            
            if len(self.meta.video_keys) > 0:
                logging.info("å†™å…¥è§†é¢‘ä¿¡æ¯...")
                self.meta.write_video_info()

            if not keep_image_files:
                img_dir = self.root / "images"
                if img_dir.is_dir():
                    logging.info("åˆ é™¤ä¸´æ—¶å›¾åƒæ–‡ä»¶...")
                    shutil.rmtree(self.root / "images")
                    
            # éªŒè¯æ–‡ä»¶åˆ›å»ºæ­£ç¡®
            logging.info("éªŒè¯æ–‡ä»¶åˆ›å»º...")
            video_files = list(self.root.rglob("*.mp4"))
            expected_video_count = self.num_episodes * len(self.meta.video_keys)
            if len(video_files) != expected_video_count:
                logging.warning(f"è§†é¢‘æ–‡ä»¶æ•°é‡ä¸åŒ¹é…! æ‰¾åˆ° {len(video_files)}, æœŸæœ› {expected_video_count}")

            parquet_files = list(self.root.rglob("*.parquet"))
            if len(parquet_files) != self.num_episodes:
                logging.warning(f"Parquetæ–‡ä»¶æ•°é‡ä¸åŒ¹é…! æ‰¾åˆ° {len(parquet_files)}, æœŸæœ› {self.num_episodes}")

            if run_compute_stats:
                logging.info("è®¡ç®—æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯...")
                self.stop_image_writer()
                self.meta.stats = compute_stats(self)
                serialized_stats = serialize_dict(self.meta.stats)
                write_json(serialized_stats, self.root / STATS_PATH)
                self.consolidated = True
                logging.info("æ•°æ®é›†æ•´åˆå®Œæˆ!")
            else:
                logging.warning(
                    "è·³è¿‡æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯è®¡ç®—ï¼Œæ•°æ®é›†æœªå®Œå…¨æ•´åˆã€‚"
                )
        except Exception as e:
            logging.error(f"æ•´åˆæ•°æ®é›†å¤±è´¥: {str(e)}\n{traceback.format_exc()}")
            raise

# ================== å¤šè¿›ç¨‹ä»»åŠ¡å¤„ç† ==================
def process_episodes_chunk(
    task_id: int,
    chunk_ids: list,
    src_path: str,
    dataset: AgiBotDataset,
    task_instruction: str,
    state_file: Path
):
    """å¤„ç†ä¸€æ‰¹episodes"""
    # è®¾ç½®æ­¤è¿›ç¨‹çš„å†…å­˜å’Œçº¿ç¨‹é™åˆ¶
    success_count = 0
    error_count = 0
    
    for episode_id in chunk_ids:
        try:
            # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡
            state = get_processing_state(state_file)
            if episode_id in state["completed_episodes"]:
                logging.info(f"è·³è¿‡å·²å¤„ç†çš„episode {episode_id}")
                continue

            # åŠ è½½æ•°æ®
            episode_data = load_local_dataset(episode_id, src_path, task_id)
            if episode_data is None:
                logging.warning(f"æ— æ³•åŠ è½½episode {episode_id}ï¼Œæ ‡è®°ä¸ºé”™è¯¯")
                update_processing_state(state_file, episode_id=episode_id, error=True)
                error_count += 1
                continue

            frames, videos = episode_data
            
            # æ·»åŠ æ‰€æœ‰å¸§
            for frame in frames:
                dataset.add_frame(frame)
            
            # ä¿å­˜episode
            success = dataset.save_episode(task=task_instruction, videos=videos)
            
            if success:
                update_processing_state(state_file, episode_id=episode_id)
                success_count += 1
                logging.info(f"æˆåŠŸå¤„ç†episode {episode_id}")
            else:
                update_processing_state(state_file, episode_id=episode_id, error=True)
                error_count += 1
                logging.warning(f"å¤„ç†episode {episode_id} å¤±è´¥")
            
            # æ¸…ç†å†…å­˜
            del frames, videos, episode_data
            
        except Exception as e:
            logging.error(f"å¤„ç†episode {episode_id} æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}\n{traceback.format_exc()}")
            update_processing_state(state_file, episode_id=episode_id, error=True)
            error_count += 1
    
    return success_count, error_count

def process_task_with_chunks(
    task_id: int,
    src_path: str,
    dataset: AgiBotDataset,
    repo_id: str,
    chunk_size: int,
    num_workers: int,
    debug: bool = False
):
    """ä½¿ç”¨åˆ†å—å¹¶è¡Œå¤„ç†å•ä¸ªä»»åŠ¡çš„æ‰€æœ‰episodes"""
    task_json = Path(src_path) / "task_info" / f"task_{task_id}.json"
    if not task_json.exists():
        logging.warning(f"ä»»åŠ¡ {task_id} çš„JSONæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
        return False

    # è·å–ä»»åŠ¡æŒ‡ä»¤
    task_instruction = get_task_instruction(str(task_json))
    logging.info(f"å¤„ç†ä»»åŠ¡ {task_id}: {task_instruction[:50]}...")

    # è·å–çŠ¶æ€æ–‡ä»¶
    state_file = get_task_state_file(repo_id, task_id)
    
    # æ ‡è®°ä»»åŠ¡å¼€å§‹å¤„ç†
    update_processing_state(state_file, start=True)

    # è·å–æ‰€æœ‰episodeç›®å½•
    episodes_dir = Path(src_path) / "observations" / str(task_id)
    if not episodes_dir.exists():
        logging.warning(f"ä»»åŠ¡ {task_id} æ²¡æœ‰observationsç›®å½•")
        return False
        
    episode_ids = [int(d.name) for d in episodes_dir.iterdir() if d.is_dir() and d.name.isdigit()]
    episode_ids.sort()
    
    # åœ¨DEBUGæ¨¡å¼ä¸‹åªå¤„ç†å‰ä¸¤ä¸ªepisode
    if debug:
        episode_ids = episode_ids[:2]
        logging.info(f"è°ƒè¯•æ¨¡å¼: åªå¤„ç†å‰ {len(episode_ids)} ä¸ªepisodes")

    # è·å–å·²å®Œæˆçš„episodes
    state = get_processing_state(state_file)
    completed_episodes = set(state["completed_episodes"])
    error_episodes = set(state["error_episodes"])
    
    # è¿‡æ»¤å‡ºæœªå¤„ç†æˆ–å‡ºé”™çš„episodes
    pending_episodes = [ep for ep in episode_ids if ep not in completed_episodes]
    
    if not pending_episodes:
        logging.info(f"ä»»åŠ¡ {task_id} æ²¡æœ‰å¾…å¤„ç†çš„episodes")
        update_processing_state(state_file, complete=True)
        return True
    
    logging.info(f"ä»»åŠ¡ {task_id}: å…± {len(episode_ids)} ä¸ªepisodes, "
                 f"å·²å®Œæˆ {len(completed_episodes)}, "
                 f"å¾…å¤„ç† {len(pending_episodes)}, "
                 f"é”™è¯¯ {len(error_episodes)}")

    # æŒ‰å—å¤„ç†episodes
    total_chunks = (len(pending_episodes) + chunk_size - 1) // chunk_size
    
    # åˆ›å»ºè¿›åº¦æ¡
    progress = tqdm(total=len(pending_episodes), desc=f"ä»»åŠ¡ {task_id} å¤„ç†è¿›åº¦")
    progress.update(len(completed_episodes))
    
    total_success = 0
    total_errors = 0
    
    # ä½¿ç”¨è¿›ç¨‹æ± å¤„ç†å—
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        futures = []
        
        # æäº¤æ‰€æœ‰å—
        for i in range(0, len(pending_episodes), chunk_size):
            chunk = pending_episodes[i:i+chunk_size]
            future = executor.submit(
                process_episodes_chunk,
                task_id,
                chunk,
                src_path,
                dataset,
                task_instruction,
                state_file
            )
            futures.append((future, len(chunk)))
        
        # ç­‰å¾…æ‰€æœ‰å—å®Œæˆå¹¶æ›´æ–°è¿›åº¦
        for future, chunk_len in futures:
            try:
                success_count, error_count = future.result()
                total_success += success_count
                total_errors += error_count
                progress.update(success_count + error_count)
            except Exception as e:
                logging.error(f"å—å¤„ç†å¤±è´¥: {str(e)}")
                total_errors += chunk_len
    
    progress.close()
    
    # æ›´æ–°æœ€ç»ˆçŠ¶æ€
    update_processing_state(state_file, complete=True)
    
    # é‡æ–°è·å–å½“å‰çŠ¶æ€
    state = get_processing_state(state_file)
    
    logging.info(f"ä»»åŠ¡ {task_id} å¤„ç†å®Œæˆ: "
                 f"æ€»episodes: {len(episode_ids)}, "
                 f"æˆåŠŸ: {len(state['completed_episodes'])}, "
                 f"é”™è¯¯: {len(state['error_episodes'])}")
    
    return True

def process_task_subset(
    task_ids: list,
    src_path: str,
    tgt_path: str,
    repo_id: str,
    chunk_size: int,
    num_workers: int,
    debug: bool = False,
    worker_id: int = 0
):
    """å¤„ç†ä¸€ç»„ä»»åŠ¡"""
    if not task_ids:
        logging.info(f"Worker {worker_id}: æ²¡æœ‰åˆ†é…ä»»åŠ¡")
        return
        
    worker_repo_id = f"{repo_id}_worker{worker_id}"
    logging.info(f"Worker {worker_id}: å¼€å§‹å¤„ç† {len(task_ids)} ä¸ªä»»åŠ¡, repo_id = {worker_repo_id}")
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = AgiBotDataset.create(
        repo_id=worker_repo_id,
        root=Path(tgt_path) / worker_repo_id,
        fps=30,
        robot_type="a2d",
        features=FEATURES,
    )
    
    # å¤„ç†æ¯ä¸ªä»»åŠ¡
    for task_id in task_ids:
        try:
            success = process_task_with_chunks(
                task_id=task_id,
                src_path=src_path,
                dataset=dataset,
                repo_id=repo_id,  # ä½¿ç”¨å…¨å±€repo_idæ¥è¿½è¸ªè¿›åº¦
                chunk_size=chunk_size,
                num_workers=num_workers,
                debug=debug
            )
            if not success:
                logging.warning(f"Worker {worker_id}: å¤„ç†ä»»åŠ¡ {task_id} å¤±è´¥")
        except Exception as e:
            logging.error(f"Worker {worker_id}: å¤„ç†ä»»åŠ¡ {task_id} å¼‚å¸¸: {str(e)}\n{traceback.format_exc()}")
    
    # æ•´åˆæ•°æ®é›†
    logging.info(f"Worker {worker_id}: æ•´åˆæ•°æ®é›†...")
    dataset.consolidate()
    logging.info(f"Worker {worker_id}: å¤„ç†å®Œæˆ")
    
    return worker_repo_id

def merge_worker_datasets(worker_datasets, tgt_path, repo_id):
    """åˆå¹¶æ‰€æœ‰å·¥ä½œå™¨çš„æ•°æ®é›†"""
    logging.info(f"å¼€å§‹åˆå¹¶ {len(worker_datasets)} ä¸ªå·¥ä½œå™¨æ•°æ®é›†...")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®é›†éœ€è¦åˆå¹¶
    valid_datasets = [ds for ds in worker_datasets if ds and Path(tgt_path) / ds]
    
    if not valid_datasets:
        logging.warning("æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®é›†éœ€è¦åˆå¹¶")
        return False
    
    if len(valid_datasets) == 1:
        # åªæœ‰ä¸€ä¸ªæ•°æ®é›†ï¼Œç›´æ¥é‡å‘½å
        src_dir = Path(tgt_path) / valid_datasets[0]
        dst_dir = Path(tgt_path) / repo_id
        
        if dst_dir.exists():
            shutil.rmtree(dst_dir)
            
        src_dir.rename(dst_dir)
        logging.info(f"åªæœ‰ä¸€ä¸ªæ•°æ®é›†ï¼Œå·²é‡å‘½åä¸º {repo_id}")
        return True
    
    # åˆ›å»ºæœ€ç»ˆæ•°æ®é›†
    final_dataset = AgiBotDataset.create(
        repo_id=repo_id,
        root=Path(tgt_path) / repo_id,
        fps=30,
        robot_type="a2d",
        features=FEATURES,
    )
    
    # TODO: å®ç°æ•°æ®é›†åˆå¹¶é€»è¾‘
    logging.warning("å¤šæ•°æ®é›†åˆå¹¶åŠŸèƒ½å°šæœªå®Œå…¨å®ç°")
    
    return False

# ================== ä¸»å¤„ç†æµç¨‹ ==================
def distributed_processing(
    src_path: str,
    tgt_path: str,
    repo_id: str = "agibotworld/all_tasks",
    debug: bool = False,
    chunk_size: int = 10,
    num_workers: int = None,
    num_processes: int = None
):
    """åˆ†å¸ƒå¼å¤„ç†ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–å…±äº«å­˜å‚¨ç›®å½•
    setup_directories()
    
    # è®¾ç½®IOä¼˜åŒ–
    setup_io_optimizations()
    
    # å¦‚æœæœªæŒ‡å®šï¼Œè‡ªåŠ¨è®¡ç®—æœ€ä½³å¹¶è¡Œåº¦
    if num_workers is None or num_processes is None:
        mem_gb = get_system_total_memory_gb()
        auto_processes, auto_workers = optimize_parallelism(mem_gb)
        
        if num_processes is None:
            num_processes = auto_processes
            
        if num_workers is None:
            num_workers = auto_workers
    
    logging.info(f"ä½¿ç”¨ {num_processes} ä¸ªè¿›ç¨‹ï¼Œæ¯ä¸ªè¿›ç¨‹ {num_workers} ä¸ªå·¥ä½œçº¿ç¨‹")
    
    # å¼ºåˆ¶è®¾ç½®huggingfaceé…ç½®
    from datasets import config
    config.HF_DATASETS_CACHE = os.environ["HF_DATASETS_CACHE"]
    
    # è‡ªåŠ¨å‘ç°æ‰€æœ‰task_id
    task_info_dir = Path(src_path) / "task_info"
    task_files = list(task_info_dir.glob("task_*.json"))
    if not task_files:
        raise ValueError("task_infoç›®å½•ä¸­æ‰¾ä¸åˆ°ä»»åŠ¡æ–‡ä»¶")
    
    task_ids = sorted([int(f.stem.split("_")[1]) for f in task_files])
    logging.info(f"å‘ç° {len(task_ids)} ä¸ªä»»åŠ¡: {task_ids}")
    
    # è·å–è¿›åº¦æƒ…å†µ
    progress = get_all_tasks_progress(repo_id, task_ids)
    
    # åˆ†æè¿›åº¦
    completed_tasks = [tid for tid, info in progress.items() 
                         if info["completed"] > 0 and not info["in_progress"]]
    
    in_progress_tasks = [tid for tid, info in progress.items() 
                         if info["in_progress"]]
    
    pending_tasks = [tid for tid in task_ids 
                       if tid not in completed_tasks and tid not in in_progress_tasks]
    
    logging.info(f"ä»»åŠ¡åˆ†æ: å·²å®Œæˆ {len(completed_tasks)}, "
                 f"è¿›è¡Œä¸­ {len(in_progress_tasks)}, "
                 f"å¾…å¤„ç† {len(pending_tasks)}")
    
    if debug:
        pending_tasks = pending_tasks[:min(2, len(pending_tasks))]
        logging.info(f"è°ƒè¯•æ¨¡å¼: ä»…å¤„ç† {len(pending_tasks)} ä¸ªä»»åŠ¡")
    
    if not pending_tasks and not in_progress_tasks:
        logging.info("æ‰€æœ‰ä»»åŠ¡å·²å¤„ç†å®Œæˆ")
        return
    
    # å°†ä»»åŠ¡åˆ†é…ç»™è¿›ç¨‹
    tasks_to_process = pending_tasks + in_progress_tasks
    logging.info(f"å°†å¤„ç† {len(tasks_to_process)} ä¸ªä»»åŠ¡")
    
    # å¦‚æœä»»åŠ¡å°‘äºè¿›ç¨‹æ•°ï¼Œè°ƒæ•´è¿›ç¨‹æ•°
    if len(tasks_to_process) < num_processes:
        num_processes = max(1, len(tasks_to_process))
        logging.info(f"ä»»åŠ¡æ•°å°‘äºè¿›ç¨‹æ•°ï¼Œè°ƒæ•´ä¸ºä½¿ç”¨ {num_processes} ä¸ªè¿›ç¨‹")
    
    # å‡åŒ€åˆ†é…ä»»åŠ¡
    task_chunks = []
    for i in range(num_processes):
        # ä½¿ç”¨äº¤é”™åˆ†é…ï¼Œç¡®ä¿æ¯ä¸ªè¿›ç¨‹åˆ†é…åˆ°ä¸åŒçš„ä»»åŠ¡ç±»å‹
        process_tasks = tasks_to_process[i::num_processes]
        if process_tasks:  # åªæ·»åŠ éç©ºä»»åŠ¡åˆ—è¡¨
            task_chunks.append(process_tasks)
    
    # å¯åŠ¨èµ„æºç›‘æ§
    monitor = ResourceMonitor(
        log_interval=60,
        log_file=Path(tgt_path) / f"{repo_id.replace('/', '_')}_resources.log"
    )
    monitor.start()
    
    try:
        # ä½¿ç”¨è¿›ç¨‹æ± å¤„ç†ä»»åŠ¡
        worker_datasets = []
        if num_processes > 1:
            with ProcessPoolExecutor(max_workers=num_processes) as executor:
                futures = []
                
                # æäº¤æ‰€æœ‰å·¥ä½œå™¨ä»»åŠ¡
                for i, tasks in enumerate(task_chunks):
                    future = executor.submit(
                        process_task_subset,
                        tasks,
                        src_path,
                        tgt_path,
                        repo_id,
                        chunk_size,
                        num_workers,
                        debug,
                        i
                    )
                    futures.append(future)
                
                # ç­‰å¾…æ‰€æœ‰å·¥ä½œå™¨å®Œæˆ
                for future in as_completed(futures):
                    try:
                        worker_repo_id = future.result()
                        worker_datasets.append(worker_repo_id)
                    except Exception as e:
                        logging.error(f"å·¥ä½œå™¨å¤„ç†å¤±è´¥: {str(e)}\n{traceback.format_exc()}")
        else:
            # å•è¿›ç¨‹æ¨¡å¼
            worker_repo_id = process_task_subset(
                task_chunks[0],
                src_path,
                tgt_path,
                repo_id,
                chunk_size,
                num_workers,
                debug,
                0
            )
            worker_datasets.append(worker_repo_id)
        
        # åˆå¹¶æ‰€æœ‰å·¥ä½œå™¨æ•°æ®é›†
        merge_worker_datasets(worker_datasets, tgt_path, repo_id)
        
        logging.info(f"è½¬æ¢å®Œæˆã€‚æ•°æ®é›†å·²ä¿å­˜åˆ°: {Path(tgt_path)/repo_id}")
        
    except KeyboardInterrupt:
        logging.info("æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
    except Exception as e:
        logging.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}\n{traceback.format_exc()}")
    finally:
        # åœæ­¢èµ„æºç›‘æ§
        monitor.stop()

# ================== å‘½ä»¤è¡Œå…¥å£ ==================
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--src_path",
        type=str,
        required=True,
        help="æºæ•°æ®ç›®å½•è·¯å¾„"
    )
    parser.add_argument(
        "--tgt_path", 
        type=str,
        required=True,
        help="è½¬æ¢åæ•°æ®é›†çš„è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--repo_id",
        type=str,
        default="lerobotV2_AgiBotWorld_sample",
        help="æ•°æ®é›†çš„HFå­˜å‚¨åº“ID"
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼ˆæ¯ä¸ªä»»åŠ¡ä»…å¤„ç†2ä¸ªepisodesï¼‰"
    )
    parser.add_argument(
        "--chunk_size",
        type=int,
        default=10,
        help="ä¸€æ¬¡å¤„ç†çš„episodesæ•°é‡"
    )
    parser.add_argument(
        "--num_processes",
        type=int,
        default=None,
        help="è¦ä½¿ç”¨çš„è¿›ç¨‹æ•°ï¼ˆé»˜è®¤è‡ªåŠ¨æ£€æµ‹ï¼‰"
    )
    parser.add_argument(
        "--num_workers",
        type=int,
        default=None,
        help="æ¯ä¸ªè¿›ç¨‹çš„å·¥ä½œçº¿ç¨‹æ•°ï¼ˆé»˜è®¤è‡ªåŠ¨æ£€æµ‹ï¼‰"
    )

    # é…ç½®æ—¥å¿—
    log_format = "%(asctime)s [%(levelname)s] [%(processName)s] %(message)s"
    logging.basicConfig(
        level=logging.INFO,
        format=log_format,
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(f"convert_{time.strftime('%Y%m%d_%H%M%S')}.log")
        ]
    )

    # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    hostname = socket.gethostname()
    cpu_cores = os.cpu_count()
    total_memory = get_system_total_memory_gb()
    logging.info(f"ä¸»æœºå: {hostname}, CPUæ ¸å¿ƒ: {cpu_cores}, å†…å­˜: {total_memory:.1f}GB")
    
    args = parser.parse_args()
    
    # éªŒè¯æºè·¯å¾„
    if not Path(args.src_path).exists():
        raise ValueError(f"æºè·¯å¾„ {args.src_path} ä¸å­˜åœ¨")
        
    # è¿è¡Œåˆ†å¸ƒå¼å¤„ç†
    distributed_processing(
        src_path=args.src_path,
        tgt_path=args.tgt_path,
        repo_id=args.repo_id,
        debug=args.debug,
        chunk_size=args.chunk_size,
        num_workers=args.num_workers,
        num_processes=args.num_processes
    )